# Imputation
Introduction
All machine learning models are categorized as either supervised or unsupervised. Supervised learning involveslearning a function that maps an input to an output based on example input-output pairs. Supervised modelsare then sub-categorized as a regression or classification model. In regression models, the result is continuous.Linear regression is merely finding a line that best fits the data. Decision trees are a popular model used inoperations research, strategic planning, and machine learning. The last node of the decision tree, where adecision is made, is called the tree leaves. (Terence Shin - towards data science, 2020)
Extreme Gradient Boosting is a model that creates a partition tree to make predictions on class-level outcomesusing data subsets. New, subsequent partition trees are applied to the remaining batches of the dataset untilresidual error is minimized. The weight of each sample batch is adaptively changed after each round of boosting(new tree). The model focuses on building trees to correctly explain data contributing to incorrect classifications.This is repeated until optimal performance is obtained. However, XGBoost is prone to over-fitting.
Support Vector Machine, is a supervised classification technique that can get pretty complicated but is intuitiveat the most fundamental level. A support vector machine will find a hyperplane or a boundary between the theclasses of data that maximizes the margin between the the classes. Many planes can separate the classes, butonly one plane can maximize the margin or distance between the classes. This plane becomes the optimalsolution for the model.
The third model type, Random Forest, is an ensemble learning technique that builds off of decision trees.Random forests involve creating multiple decision trees using bootstrapped datasets of the original data andrandomly selecting a subset of variables at each decision tree step. Relying on a "majority wins" model reducesthe risk of error from an individual tree.
We will train all three models in this study and vary the hyperparameters to maximize the models' accuracyvalues and minimize log-loss while considering the runtime requirements.
